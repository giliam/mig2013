\chapter{Modélisation des mots à reconnaître par les modèles de Markov cachés}
	\section{Objectifs}
	    Le traitement effectué sur le son permet d'obtenir un tableau fréquentiel caractéristique du mot. Nous pouvons alors rentrer dans le vif du sujet et travailler sur la reconnaissance même des mots : il faut parvenir à comparer entre eux les
	    empreintes ainsi obtenues. Pour cela, nous utilisons un système appelé les \emph{modèles de Markov cachés}. Nous créons en fait un \og graphe de sons \fg qui peut être parcouru à partir d'un mot. En appliquant un certain algorithme à un mot et à un graphe, on obtient une probabilité qui témoigne de l'adéquation du mot au graphe. 
	    Il nous suffit alors de déterminer quel graphe représente la plus grand probabilité ce qui nous donne la solution comme étant le \emph{meilleur candidat}. On parle alors de \textbf{maximum de vraisemblance}.
        \section{Prérequis et principe}
        \paragraph{}
        	Un modèle de Markov caché est un modèle statistique qui peut modéliser des processus physiques. Il fait appel aux structures d'automates\cite{automate}.
		\subsection{Les automates}
Un automate représente un système physique. Il est composé d'états (les cercles sur la figure), qui correspondent aux états du système réel, 
et de transitions (les flèches sur la figure), pour passer d'un état à l'autre. 
Il existe aussi la notion de chemin: par exemple pour passer de 0 à 3 sur la figure, il faut passer par 1 puis 2: le chemin de 0 à 3 est 0,1,2,3.

				\begin{figure}[hb]
						\begin{center}
							\includegraphics[width=10cm]{Images/MMC1.png} 
						\end{center}
						\caption{Exemple d'automate \og classique \fg}
				\end{figure}
\subsection{Les modèles de Markov cachés}
\paragraph{}
Un modèle de Markov est un automate présentant \emph{deux caractéristiques} en plus du principe de base d'un automate. 
\paragraph{}
Tout d'abord, les transitions ne sont plus déterministes comme elles le sont dans le cas d'un automate mais sont \textbf{probabilistes}. Ainsi, il y a une certaine probabilité pour passer à chacun des autres état, une fois arrivé dans un état lors d'un parcours. C'est pourquoi le déplacement dans un automate ne dépend plus du passé du parcours mais uniquement de la position actuelle. 
\paragraph{}
La seconde différence est que les données renvoyées lors du parcours d'un chemin de l'automate n'est plus la liste des états par lesquels passe le chemin : chaque état a maintenant des probabilités d'émettre certains signaux. On obtient donc une liste de signaux pour un chemin. Or, un état peut émettre plusieurs signaux différents et l'émission n'est que probabiliste ce qui rajoute au caractère non déterministe de l'automate.
\begin{figure}[hb]
    \begin{center}
	    \includegraphics[width=10cm]{Images/mmcpresentation.png} 
    \end{center}
    \caption{Exemple d'un modèle de Markov caché}
\end{figure}
\paragraph{}
La spécificité des modèles de Markov cachés qui les rend si utiles en reconnaissance vocale est le fait qu'ils puissent apprendre et se perfectionner. En effet, il est possible de démontrer qu'en appliquant certaines formules sur l'automate à partir d'un mot (données par l'algorithme de Baum-Welch\cite{rabiner}\cite{baum}), on parvient à l'améliorer et à le faire converger dans le domaine des automates vers un automate reconnaissant plus fidèlement le mot qu'on lui applique.
\subsection{Modèles discrets et modèles continus}
La figure qui vous a été présentée ci-dessus a un nombre fini de signaux, il s'agit de ce qu'on appelle un modèle de Markov caché \emph{discret}. Il existe une version \emph{continue} de ces automates où l'on remplace les signaux par des fonctions continues d'un espace de dimension supérieure ou égale à 1.
Lorsqu'un état émet un signal, au lieu de chercher entre les différents signaux possibles, il effectue un calcul sur une combinaison linéaires de fonctions gaussiennes dans l'espace à plusieurs dimensions. Les pics des gaussiennes représenteraient les signaux discrets. La nature des gaussiennes regroupe donc les probabilités autour de ces pics en conservant le caractère continu des fonctions.
\subsection{Application à la reconnaissance vocale}
\paragraph{}
Les modèles de Markov cachés sont largement répandus dans la reconnaissance vocale\cite{rabiner}\cite{ref1}\cite{calcvoc}. 
Entre un modèle discret et un modèle continu, nous avons choisi ce dernier car les données en entrée ne font pas partie d'un ensemble fini : 
il existe une infinité de sons possibles pour un même phonème. 
Les modèles de Markov cachés sont particulièrement adaptés pour la reconnaissance vocale car 
ils permettent un apprentissage constant de la part du programme : 
celui-ci est capable d'apprendre de nouveaux mots de manière autonome, et de s'améliorer au-fur-et-à-mesure que la base de données de mots grandit.
\paragraph{}
Nous avons modélisé chaque mot par un automate, dont les états sont les différents phonèmes du mot. 
Lorsque l'on prononce un mot, on se dirige dans l'automate grâce aux phonèmes prononcés, 
jusqu'à rencontrer l'état final. Ceci permet de reconnaître le mot même si une syllabe dure 
plusieurs secondes: dans ce cas, on se contente de tourner en rond  
(en restant sur l'état 0 de la figure par exemple) dans l'automate jusqu'à rencontrer un nouveau phonème. 
Dans l'automate, la transition de l'état $i$ à $k$ représente la probabilité de passer de l'état $i$ à $k$, 
c'est-à-dire la probabilité que le phonème n°$k$ vienne tout de suite après le phonème n°$i$.

				\begin{figure}[H]
						\begin{center}
							\includegraphics[width=10cm]{Images/MMC2.png} 
						\end{center}
						\caption{Exemple de deux phonèmes et de la probabilité de passer du phonème 1 au phonème 2}
				\end{figure}
		\section{Principaux algorithmes sur les modèles de Markov}
\paragraph{}
Lorsque l'on fait passer un mot dans un automate, ie. qu'on s'oriente dans l'automate à l'aide des phonèmes, on peut calculer la probabilité que le mot corresponde à cet automate: 
on multiplie toutes les probabilités rencontrées pendant le parcours. Elles dépendent bien sûr du chemin parcouru 
(i-e des transitions rencontrées). 
C'est le principe de l'algorithme \emph{forward}.
\paragraph{}
L'algorithme de \emph{Baum-Welch} permet d'optimiser un automate. En se plaçant dans l'ensemble des modèles de Markov, on cherche à faire converger une suite
d'automates définis à l'aide de plusieurs versions d'un même mot vers un automate optimisé qui corresponde au mieux au mot.


		\section{Application à notre objectif}
\paragraph{}
		Résumons la situation lorsque l'on lance notre programme: d'un côté une base de données de mots, représentés chacun par un automate ; 
de l'autre, un fichier audio: le mot prononcé par l'utilisateur. Le programme se déplace dans chaque automate grâce au fichier audio, 
il s'oriente en fonction des phonèmes prononcés. Nous appellerons cette opération "faire passer un mot dans un automate".
\paragraph{}
L'algorithme \emph{forward} permet donc de calculer 
la probabilité qu'un automate corresponde au mot prononcé: en comparant les probabilités dans chacun des automates, 
on sélectionne la plus grande et on a l'automate qui correspond le mieux au mot sélectionné.  
\paragraph{}
L'algorithme de Baum-Welch permet l'apprentissage de nouveaux mots: pour chaque nouveau mot il crée un nouvel automate, 
et le rend le plus optimisé possible en s'appuyant sur la bibliothèque existante. C'est ce que fait la partie logicielle de notre programme, 
pour que les programmeurs puissent agrandir la base de données.
        \section{Phase d'apprentissage}
        \paragraph{}
        Une fois l'algorithme de reconnaissance vocale implémenté, il nous a fallu l'améliorer. 
        Deux aspects demandent un apprentissage de la part du programme. 
        Il doit d'abord faire grossir l'ensemble des mots reconnus, de manière à pouvoir en reconnaître le plus possible. 
        Mais il est aussi intéressant de lui faire apprendre un mot par des locuteurs différents. 
        Plus le nombre de locuteurs est grand, plus l'algorithme peut être précis.
        \paragraph{}
Enregistrer plusieurs personnes permet d'obtenir une diversité de spectres qui accroît la précision du programme.
\paragraph{}
Une fois un mot appris, il est également très utile qu'un même locuteur enregistre de nombreuses versions du mot. Nous avons fait pour notre locuteur 10 versions de chaque mot.
\paragraph{}
Pour mettre en place un apprentissage, nous avions des besoins matériels (stocker l'ensemble des mots reconnus) mais aussi des besoins humains, et en l'occurrence une diversité de voix.
        
        \section{Phase de reconnaissance}
        \paragraph{}
La phase de reconnaissance constitue le c\oe{}ur du programme. Comme dit précédemment, le programme effectue l'algorithme \emph{forward} sur chacun des automates et renvoie le mot le plus probable, après avoir comparé toutes les probabilités.
\paragraph{}
A l'origine, la phase de reconnaissance a été codée en Python. Cependant le temps d'exécution étant trop long, nous l'avons donc codé en C++, ce qui a permis de diviser le temps d'exécution par 400. Grâce à ce travail laborieux, le programme s'effectue en un temps proche de la seconde. Tout a été mis en place, notamment en amont avec le codage en C++ de la transformée de Fourier rapide, pour privilégier la rapidité de l'exécution. 
\paragraph{}
Au départ nous n'avions qu'un seul locuteur pour faire la base de donnée des mots reconnus, 
ce qui ne permettait de faire fonctionner le programme que pour un seul utilisateur : celui qui avait enregistré les mots. 
Cependant nous avons enregistré plusieurs locuteurs, ce qui permet au programme de reconnaître plusieurs utilisateurs,
 même un utilisateur qui n'aurait pas encore enregistré de mot.
 \section{Récapitulatif du principe de création du modèle de Markov caché}
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=10cm]{Images/mmcrecap.png} 
	\end{center}
	\caption{Exemple de deux phonèmes et de la probabilité de passer du phonème 1 au phonème 2}
\end{figure}